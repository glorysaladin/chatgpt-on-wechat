# -*- coding: utf-8 -*-
import sys, re, os, time, json, re
from datetime import datetime
import html5lib
from bs4 import BeautifulSoup
import traceback
import urllib
import requests
import random
os.environ['NO_PROXY'] = 'affdz.com,moviespace01.com,moviespace02.com'

cur_dir=os.path.dirname(__file__)
sys.path.append(cur_dir)

from get_pan_from_funletu import *
from get_pan_from_uukk import *
from get_movie_from_soupian import *
from get_movie_from_zhuiyingmao import *

headers={"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.41"}

def download(url):
    proxies = {"http":None, "https":None}
    resp = requests.get(url, headers=headers, proxies=proxies)
    return resp.text


def _extract_update(httpDoc, pattern='json'):
    soup = None
    try:
        soup = BeautifulSoup(httpDoc, 'html5lib')
    except:
        soup = BeautifulSoup(httpDoc, 'html.parser')
    htmlNode = soup.html
    headNode = htmlNode.head
    bodyNode = htmlNode.body
    itemNodes = bodyNode.find_all(attrs={"class": "sou-con"})
    rets = {}
    for item in itemNodes:
        divNodes = item.find_all(attrs={"class":"sou-con-title"})
        hit=False
        for div in divNodes:
            if "æŒç»­æ›´æ–°" in div.text:
                hit = True
                break
        if hit:
            aNodes = item.find_all('a')
            for item in aNodes:
                if item.has_key("title") and item.has_key('href'):
                     href = item['href']
                     title = item['title']
                     if "post" not in href:
                         continue
                     rets[title]=href
    return rets


def _extract_movie_info(httpDoc, pattern='json'):
    soup = None
    try:
        soup = BeautifulSoup(httpDoc, 'html5lib')
    except:
        soup = BeautifulSoup(httpDoc, 'html.parser')
    htmlNode = soup.html
    bodyNode = htmlNode.body
    itemNodes = bodyNode.find_all('a')
    title_postid_map = {}
    for item in itemNodes:
        if item.has_key("title") and item.has_key('href'):
            href = item['href']
            title = item['title']
            if "post" not in href:
                continue
            title_postid_map[title] = href
    return title_postid_map

def get_source_link(url):
    title_text = ""
    try:
        proxies = {"http":None, "https":None}
        i = 0
        httpDoc=""
        while i < 3:
            try:
                resp = requests.get(url, headers=headers, proxies=proxies, timeout=10)
                httpDoc = resp.text
                resp.close()
                break
            except:
                i+=1

        soup = None
        try:
            soup = BeautifulSoup(httpDoc, 'html5lib')
        except:
            soup = BeautifulSoup(httpDoc, 'html.parser')
        htmlNode = soup.html
        headNode = htmlNode.head
        bodyNode = htmlNode.body
        itemNodes = bodyNode.find_all(attrs={"class": "sou-con"})
        titleNode = bodyNode.find('h1', attrs={"class": "detail_title"})
        if titleNode is not None:
            title_text = titleNode.text

        contentNode = bodyNode.find('div', attrs={"class":"article-content"})
        sourceLinks = contentNode.find_all('a')
        for link in sourceLinks:
            if "http" in link["href"]:
                return link["href"], title_text
    except:
        print(traceback.format_exc())
    return "", title_text

def get_latest_postid(last_post_id, web_url):
    rets = {}
    html_page=download(web_url)
    ret_page=_extract_movie_info(html_page)
    rets.update(ret_page)

    max_post_id = -1
    for key in rets:
        href = rets[key]
        cur_id = int(href.split("/")[-1].split(".")[0])
        if cur_id > max_post_id:
            max_post_id = cur_id
    return max_post_id

def get_movie_update(last_post_id):
    rets = {}
    urls = ["https://affdz.com", "https://affdz.com/page_2.html"]
    for url in urls:
        html_page=download(url)
        ret_page=_extract_movie_info(html_page)
        rets.update(ret_page)

    max_post_id = -1
    movie_list = []
    for key in rets:
        href = rets[key]
        cur_id = int(href.split("/")[-1].split(".")[0])
        #print(key, href, cur_id)
        if cur_id > max_post_id:
            max_post_id = cur_id
        if cur_id > last_post_id:
            movie_list.append(key)
    print("1", movie_list)

    html = download("https://affdz.com/tags-1.html")
    update_rets = _extract_update(html)
    for title in update_rets:
        movie_list.append(title)
    print("2", movie_list)

    rets.update(update_rets)
    #print(rets, max_post_id, last_post_id)

    #movie_list = list(set(movie_list))
    message_list =[]
    prefix = "ã€ä»Šæ—¥å½±è§†èµ„æºæ›´æ–°ã€‘"
    message_list.append(prefix)

    idx = 0
    exist_map={}
    for movie in movie_list:
        if movie in exist_map:
            continue
        exist_map[movie]=1
        link = ""
        if movie in rets:
            link, title_text = get_source_link(rets[movie])
            if link == "":
                link = rets[movie]
        message_list.append("{}: {}\né“¾æ¥: {}".format(idx, movie, link))
        idx += 1

    message_list.append("å…¨éƒ¨èµ„æºé“¾æ¥:\n https://sourl.cn/XeN2ex\nå¤¸å…‹ç½‘ç›˜SVIPä¼šå‘˜(12å…ƒ)\nhttps://sourl.cn/vAxErZ \nè”ç³»ç¾¤ä¸».")

    if len(movie_list) == 0:
        print("no update film.")
        sys.exit()

    message = "\n".join(message_list)
    return (max_post_id, message)

def get_random_movie(start_post, end_post, rand_num, base_url, show_link=False):
    rets = []
    for post_id in random.sample(range(start_post, end_post), rand_num):
        url="{}/post/{}.html".format(base_url, post_id)
        link, title = get_source_link(url) 
        if link != "":
            if not show_link:
                link = ""
            rets.append("{}\n{}".format(title, link)) 
    if not show_link and len(rets) > 0:
        rets.append("ğŸ‘‰èµ„æºå¯ä»¥ä»ç¾¤å…¬å‘Šå–")
        #rets.append("https://6url.cn/tEQs9z")
    
    return "\n".join(rets) 

def good_match(s1, s2):
    set1 = set(s1)
    set2 = set(s2)
    common_chars = set1.intersection(set2)
    if len(common_chars)*1.0  / (len(set1) + 0.1) > 0.2:
       return True 
    return False

def get_from_affdz(web_url, moviename, show_link=False):
    rets = []
    try:
        #print(f"start affdz {web_url} {moviename}")
        url="{}/search.php?q={}".format(web_url, moviename)
        proxies = {"http":None, "https":None}
        i = 0 
        httpDoc=""
        while i < 3:
            print(f"try {i} times.")
            try:
                resp = requests.get(url, headers=headers, proxies=proxies, timeout=(10,10))
                httpDoc = resp.text
                resp.close()
                break
            except requests.exceptions.RequestException as e:
                i+=1
                print("get from affdz error", e)

        soup = None
        #print(f"start affdz2 {web_url} {moviename}")
        try:
            soup = BeautifulSoup(httpDoc, 'html5lib')
        except:
            soup = BeautifulSoup(httpDoc, 'html.parser')
        htmlNode = soup.html
        bodyNode = htmlNode.body
        listNode = bodyNode.find('div', attrs={"class":"sou-con-list"})
        aNodes = listNode.find_all('a')
        source=""
        for item in aNodes:
            href = ""
            title = ""
            if item.has_attr("title") and item.has_attr('href'):
                 href = item['href']
                 if "post" not in href:
                     continue
                 title = item['title'].replace("<strong>", "").replace("</strong>", "")
            if good_match(moviename, title):
                 movieurl = href
                 link, title_text = get_source_link(href)
                 if link.strip() == "":
                     link = href.split("url=")[1].split("&")[0]
                 if not show_link:
                     link = " "
                 rets.append("{}\n{}".format(title, link))
    except:
        print("error=",traceback.format_exc())
    #print(f"start affdz3 {web_url} {moviename}")
    return rets

def _get_search_result(web_url_list, moviename, show_link, is_pay_user, only_affdz, pattern='json'):
    source = ''
    rets = []
    for idx, web_url in enumerate(web_url_list):
        rets.extend(get_from_affdz(web_url, moviename, show_link))
        if len(rets) > 0:
            source = source + str(idx)
            break

    if not only_affdz:
        if len(rets) == 0:
            rets.extend(get_from_uukk(moviename, is_pay_user))
            if len(rets) > 0:
                source += "2"

        if len(rets) == 0:
            rets.extend(get_from_funletu(moviename))
            if len(rets) > 0:
                source += "3"

    if len(rets) == 0:
        return False, ["æœªæ‰¾åˆ°èµ„æº, å¯å°è¯•ç¼©çŸ­å…³é”®è¯, åªä¿ç•™èµ„æºå, ä¸è¦å¸¦'ç¬¬å‡ éƒ¨ç¬¬å‡ é›†è°¢è°¢'ï¼Œç­‰æ— å…³è¯."]

    num = len(rets)
    rets.insert(0, "[{}]æ‰¾åˆ° {} ä¸ªç›¸å…³èµ„æº:\n".format(source, num))

    return True, rets

def search_movie(web_url_list, movie, show_link=False, is_pay_user=False, only_affdz=False):
    return _get_search_result(web_url_list, movie, show_link, is_pay_user, only_affdz)

def need_update(my_count, other_count):
    try:
        if "." in my_count and "." in other_count:
            # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡
            date1_obj = datetime.strptime(str(my_count), "%m.%d")
            date2_obj = datetime.strptime(str(other_count), "%m.%d")
            if date1_obj < date2_obj:
                return True
            return False
        else:
            if "." not in my_count and "." not in other_count:
                if int(other_count) > 2000:
                    return False
                if int(my_count) < int(other_count):
                    return True
                return False
    except:
        pass
    return False

def send_update_to_group(movie_update_data, web_url, show_link):
    curdir=os.path.dirname(os.path.abspath(__file__))
    shell_cmd =  "sh {}/get_state_from_feishu.sh".format(curdir)
    return_cmd = subprocess.run(shell_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8',shell=True)
    update_movies = []
    movie_source_map = {}
    if return_cmd.returncode == 0:
        ret_val = return_cmd.stdout
        js = json.loads(ret_val)
        values = js.get("data", {}).get("valueRange", {}).get("values", [])
        for item in values:
            if item[0] is not None and item[0] != "None" and item[0] != "null":
                movie_name = item[0].strip()
                version = str(item[1]).strip()
                if item[2] is not None:
                    source = item[2].strip() 
                    movie_source_map[movie_name] = source
                if movie_name not in movie_update_data or movie_update_data[movie_name] is None or movie_update_data[movie_name] == "None":
                    movie_update_data[movie_name] = version
                    update_movies.append(movie_name)
                else:
                    cur_version = version.replace("é›†", "")
                    last_version = movie_update_data[movie_name].replace("é›†", "")
                    if need_update(last_version, cur_version):
                        update_movies.append(movie_name)
                        movie_update_data[movie_name] = version
    msg_ret = []
    print(update_movies)
    for movie in update_movies:
        link = ""
        if movie in movie_source_map and "http" in movie_source_map[movie]:
            link = movie_source_map[movie] 
        else:
            ret = get_from_affdz(web_url, movie, True)
            if len(ret) > 0:
                items = ret[0].split("\n")
                link = items[1]
        if len(link) > 0:
            if not show_link:
                link = ""
            msg = "[{}] (æ›´æ–°åˆ°{})\n{}".format(movie, movie_update_data[movie], link)
            msg_ret.append(msg)
    print("update movies={}".format(msg_ret))
    if not show_link and len(msg_ret) > 0:
        msg_ret.append("èµ„æºé“¾æ¥å¯ä»¥ä»ç¾¤å…¬å‘Šè·å–")
        #msg_ret.append("https://6url.cn/tEQs9z")
    return "\n\n".join(msg_ret)
 
def check_update():
    update_infos=[]
    curdir=os.path.dirname(os.path.abspath(__file__))
    shell_cmd =  "sh {}/get_state_from_feishu.sh".format(curdir)
    return_cmd = subprocess.run(shell_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8',shell=True)
    movie_series =[]
    if return_cmd.returncode == 0:
        ret_val = return_cmd.stdout
        js = json.loads(ret_val)
        values = js.get("data", {}).get("valueRange", {}).get("values", [])
        for item in values:
            if item[3] is not None:
                continue
            if item[0] is not None:
                movie_series.append((item[0], str(item[1]).replace("é›†", ""), item[2]))
    print("movie_series={}".format(movie_series))
    for moviename, my_count, source in movie_series:
        rets = []
        fuletu_rets = get_from_funletu(moviename)
        #print("query=", moviename, "fuletu_rets=", fuletu_rets)
        rets.extend(fuletu_rets)
        uukk_rets = get_from_uukk(moviename, True)
        rets.extend(uukk_rets)
        for ret in rets:
            #print(ret)
            try:
                if "quark" not in ret:
                    continue
                items = ret.strip().split("\n")
                if len(items) >= 2:
                    cur_name = items[0]
                    pattern = r"[æ›´|å…¨]\D*(\d+\.?\d*)"
                    matches = re.findall(pattern, cur_name)
                    for match in matches:
                        if need_update(my_count, match):
                            update_infos.append("ã€{}ã€‘{}\nå½“å‰ ï¼ˆ{}ï¼‰--> æœ€æ–° ï¼ˆ{}ï¼‰\n{}\n".format(moviename, source, my_count, match, ret))
            except:
                print("error", ret)
    return "\n".join(update_infos)

#print(check_update())
#print(get_movie_update(1414))
#print(get_source_link("https://moviespace01.com/post/1671.html"))
#print(get_random_movie(1000, 1500, 2,"https://affdz.com"))

#movie_version="/home/lighthouse/project/chatgpt-on-wechat2/plugins/movie/movie_update_version.pkl"
#movie_update_data={}
#movie_update_data={'ä½ å¥½æ˜ŸæœŸå…­': '5.26', 'å£°ç”Ÿä¸æ¯å®¶å¹´å': '02.18', 'ä¸æ¶é­”æœ‰çº¦': '16é›†', 'æˆ‘ä»¬çš„ç¾å¥½ç”Ÿæ´»': '1.20', 'çˆ±çš„ä¿®å­¦æ—…è¡Œ': '02.07', 'èŠ±å„¿ä¸å°‘å¹´': '02.07', 'å¤©å®˜èµç¦2': '12é›†', 'æˆ‘å¯ä»¥47': '12.27', 'è¯·å’Œæˆ‘çš„è€å…¬ç»“å©š': '16é›†', 'ä½ ä¹Ÿæœ‰ä»Šå¤©': '36é›†', 'å®‹æ…ˆéŸ¶åå½•': '25é›†', 'æˆ‘ä»¬çš„ç¿»è¯‘å®˜': '36é›†', 'å¤§æ±Ÿå¤§æ²³ä¹‹å²æœˆå¦‚æ­Œ': '33é›†', 'é»‘åœŸæ— è¨€': '12', 'å¦‚æœå¥”è·‘æ˜¯æˆ‘çš„äººç”Ÿ': '28é›†', '19å±‚': '30é›†', 'åä¾¦æ¢å­¦é™¢': '02.17', 'é®å¤©': '59é›†', 'ç¥ˆä»Šæœ': '36é›†', 'ä»™å‰‘å››': '36é›†', 'è¦ä¹…ä¹…çˆ±': '32é›†', 'å°åŸæ•…äº‹å¤š': '30é›†', 'æ€äººè€…çš„è´­ç‰©ä¸­å¿ƒ': '08é›†', 'ä¹Œæœ‰ä¹‹åœ°': '1.26', 'å¥½ä¹…æ²¡åš': '06é›†', 'ç‹—å‰©å¿«è·‘': '24é›†', 'ç©ºæˆ˜ç¾¤è‹±': '08é›†', 'å¤§ç‹é¥¶å‘½': '12é›†', 'åå™¬æ˜Ÿç©º': '121é›†', 'èµµæœ¬å±±å°å“åˆé›†': '2.11', 'æ¬¢ä¹å–œå‰§äººå°å“åˆé›†': '1.28', 'é»„å®ã€æ½˜é•¿æ±Ÿå°å“åˆé›†': '1.28', 'å†¯å·©ã€éƒ­å†¬ä¸´ç›¸å£°å°å“åˆé›†': '1.28', 'èµµä¸½è“‰å°å“åˆé›†': '1.28', 'é™ˆä½©æ–¯ã€æœ±æ—¶èŒ‚å°å“åˆé›†': '1.28', 'éƒ­å¾·çº²ç›¸å£°': '1.28', 'æ˜¥èŠ‚è”æ¬¢æ™šä¼šç›¸å£°å°å“åˆé›†': '1.28', 'å†å¹´æ˜¥æ™šå°å“ç›¸å£°åˆé›†216é›†': '2.11', '50éƒ¨æ˜¥æ™šå°å“å¤§å…¨': '2.11', 'é‡‘ç“¶æ¢…åŸè‘—ã€å…¨å½©æ’å›¾ã€‘ã€æœªåˆ å‡ã€‘': '1.28', '800æœ¬è¿ç¯ç”»å„¿æ—¶è®°å¿†3': '1.28', '800æœ¬è¿ç¯ç”»å„¿æ—¶è®°å¿†2': '1.28', '800æœ¬è¿ç¯ç”»å„¿æ—¶è®°å¿†1': '1.28', 'å››å¤§åè‘— è¿ç¯ç”»': '1.28', 'é‡‘åº¸å°è¯´æ¼«ç”»': '1.28', 'ä¼Šè—¤æ¶¦äºŒåˆé›†': '1.28', 'èŠæ–‹æ•…äº‹è¿ç¯ç”»': '1.28', 'é«˜æ¸…æ€€æ—§è¿ç¯ç”»': '1.28', 'é‡‘ç“¶æ¢…ï¼ˆå…¨å½©è¿ç¯ç”»ç‰ˆï¼‰ç»ç‰ˆå½©è‰²å›½ç”»ç»å…¸çè—': '1.28', 'ç½‘æ˜“äº‘è¯„è®ºæœ€å¤šçš„ä¸­æ–‡æ­Œæ›²TOP50': '1.28', 'ç½‘æ˜“äº‘è¯„è®ºæœ€å¤šçš„ç²¤è¯­æ­Œæ›²TOP100': '1.28', 'ç½‘æ˜“äº‘è¯„è®ºæœ€å¤šçš„è‹±æ–‡TOP100': '1.28', 'ç½‘æ˜“äº‘è¯„è®ºæœ€å¤šçš„æ—¥è¯­TOP100': '1.28', 'ç½‘æ˜“äº‘è¯„è®ºæœ€å¤šçš„éŸ©è¯­æ­Œæ›²TOP200': '1.28', 'ç½‘æ˜“äº‘è¯„è®ºæœ€å¤šçš„çº¯éŸ³ä¹TOP100': '1.28', 'å…¨ç½‘é¡¶çº§AIç»˜ç”»æå“ç¾å¥³': '1.28', '19xç”µå½±åˆé›†70éƒ¨': '1.28', 'è±†ç“£è¯„åˆ†Top20': '1.28', 'éŸ©å›½æœ€å‡ºè‰²çš„Ré™åˆ¶ã€åˆé›†ã€‘': '1.28', 'å½±è¿·æŠ•ç¥¨é€‰å‡ºäº†è¿‘åå¹´ä»–ä»¬æœ€å–œæ¬¢çš„50éƒ¨ææ€–ç‰‡ï¼': '1.28', 'äººäººå½±è§†ç”µå½±åˆé›†--é«˜æ¸…åŒè¯­å­—å¹•ï¼ˆçè—ç‰ˆï¼‰': '1.28', 'å®«å´éªä½œå“åˆé›†': '1.28', 'çš®å…‹æ–¯åŠ¨ç”»åˆé›†': '1.28', 'ç»å…¸é¦™æ¸¯ç”µå½±åˆé›†ï¼ˆä¿®å¤æœªåˆ å‡ç‰ˆæœ¬ï¼‰': '1.28', 'è¿ªå£«å°¼ç³»åˆ—åŠ¨ç”»139éƒ¨è“å…‰çè—ç‰ˆ': '1.28', 'å‘¨æ˜Ÿé©°ç³»åˆ—': '1.28', 'DCç”µå½±å®‡å®™ç³»åˆ—': '1.28', 'æ¼«å¨ç”µå½±å®‡å®™(MCU)ç³»åˆ—': '1.28', 'ä¸€ç™¾å¹´ä¸€ç™¾éƒ¨ ç¾å›½ç”µå½±å­¦é™¢ç™¾å¹´ç™¾éƒ¨å½±ç‰‡': '1.28', '2023å¥¥æ–¯å¡æåç”µå½±åˆé›†(95å±Š)': '1.28', 'å¥¥æ–¯å¡è·å¥–å½±ç‰‡1988-2022': '1.28', 'è±†ç“£ç”µå½±Top250': '1.28', 'æ¬¢ä¹å®¶é•¿ç¾¤': '40é›†', 'é˜¿éº¦ä»å†›': '36é›†', 'æš´é›ªæ—¶åˆ†': '06é›†', 'åœ¨æš´é›ªæ—¶åˆ†': '30é›†', 'å°˜å°åä¸‰è½½': '24é›†', 'æˆå¥¹ä»¥æŸ„': '20é›†', 'å°‘çˆ·å’Œæˆ‘': '12é›†', 'æ–—ç ´è‹ç©¹å¹´ç•ª': '95é›†', 'ç‹¬ä¸€æ— äºŒçš„å¥¹': '24 é›†', '2024æ¹–å—å«è§†æ˜¥èŠ‚è”æ¬¢æ™šä¼š': '2.5', 'åŒå¿ƒå‘æœªæ¥Â·2024ä¸­å›½ç½‘ç»œè§†å¬å¹´åº¦ç››å…¸': '2.5', 'ä¸–ç•Œå„åœ°çš„æ€§ä¸çˆ±  (2018) ä¸­æ–‡å­—å¹•': '2.5', 'å°æ¹¾é£Ÿå ‚ä¸‰å­£': '2.5', 'ä¸–ç•Œå„åœ°çš„æ€§ä¸çˆ±': '2.5', 'å¤§å”ç‹„å…¬æ¡ˆ': '32é›†', 'å—æ¥åŒ—å¾€': '39é›†', 'é»‘ç™½æ½œè¡Œ': '2.7', 'çˆ±çˆ±å†…å«å…‰(å°å‰§)': '8é›†', 'ç”œç”œçš„é™·é˜±': '24é›†', 'å†å±Šæ˜¥æ™šå°å“ç›¸å£°': '2.11', 'ä»™é€†': '38é›†', 'é£é©°äººç”Ÿ': '2.13', 'å¦‚æ‡¿ä¼ ': '87é›†', 'çƒŸç«äººé—´': '18é›†', 'xianh\u2006xun\u2006ai\u2006qing': 'None', 'ä¹¡æ‘çˆ±æƒ…16': '40é›†', 'åŠ¨ç‰©å›­é‡Œæœ‰ä»€ä¹ˆ': '2.16', 'ç¼‰æ¶': '2.16', 'å¤§ä¾¦æ¢ç¬¬ä¹å­£': '5.08', 'å…¨å‘˜åŠ é€Ÿä¸­': '4.18', 'çƒŸç«äººå®¶': '41é›†', 'é‡‘æ‰‹æŒ‡': '2.2', 'å¤§ç†å¯ºå°‘å¿æ¸¸': '36é›†', 'æ˜¥æ—¥æµ“æƒ…  14é›†': 'None', 'å¸ˆå…„å•Šå¸ˆå…„ 24é›†': 'None', 'å†°ç«é­”å¨': '138é›†', 'æ˜¥æ—¥æµ“æƒ…': '17é›†', 'å¸ˆå…„å•Šå¸ˆå…„': '26é›†', 'çŒå†°': '18é›†', 'å¤§ç†å¯ºå°‘å¿': '12é›†', 'å—æ¥åŒ—å¾€2': '39é›†', 'æ€’æ½®': '2.22', 'ç§åœ°å§ç¬¬äºŒå­£': '2.23', 'æš®è‰²å¿ƒè¿¹': '24é›†', 'æ˜æ˜Ÿå¤§ä¾¦æ¢ç¬¬ä¹å­£': '3.02', 'ç‚™çˆ±ä¹‹æˆ˜': '2.14', 'çŒå†°2': '09é›†', 'è¡Œå°¸èµ°è‚‰ï¼šå­˜æ´»ä¹‹äºº': '06é›†', 'æˆ‘æƒ³å’Œä½ å”±ç¬¬5å­£': '2.24', 'ä½ çš„å²›å±¿å·²æŠµè¾¾': '12é›†', 'çŠ¯ç½ªç°åœº4': '10é›†', 'æˆ¿ä¸œåœ¨ä¸Š': '98é›†', 'å¤§ç‹åˆ«æ…Œå¼ ': '14é›†', 'å¤§ä¸»å®°': '50é›†', 'åŠç†Ÿæ‹äºº3': '5.02', 'æ— é™è¶…è¶Šç­2': '5.11', 'ä¹å¯å¹¿æ’­å‰§': '2.28', 'å©šåäº‹': '14é›†', 'æ±Ÿæ²³æ—¥ä¸Š': '24é›†', 'å¹•åºœå°†å†›': '10é›†', 'å”äººè¡—æ¢æ¡ˆ2å‰§ç‰ˆ': '03é›†', 'å…‰ç¯ç¬¬äºŒå­£': '07é›†', 'ä¸œäº¬ç½ªæ¶2': '05é›†', 'å‘¨å¤„é™¤ä¸‰å®³': '3.1', 'é‡‘å­—å¡”æ¸¸æˆ': '06é›†', 'å¤§ç†å¯º å°‘å¿æ¸¸': '22', 'é£é©°äººç”Ÿçƒ­çˆ±ç¯‡': '10é›†', 'åˆ«å¯¹æˆ‘åŠ¨å¿ƒ': '18é›†', 'å”äººè¡—æ¢æ¡ˆ2': '16é›†', 'ç§åœ°å§2024': '4.28', 'è‘¬é€çš„èŠ™è‰è²': '28é›†', 'ä»™æ­¦å¸å°Š': '78é›†', 'æ°¸å®‰æ¢¦': '24é›†', 'ç ´å¢“': '4.23', 'è¿‘æˆ˜æ³•å¸ˆ': '20é›†', 'ç´«å·Â·å…‰æ˜ä¸‰æ°': '24é›†', 'ä¸€æ¢¦æµ®ç”Ÿ': '22é›†', 'çœ¼æ³ªå¥³ç‹': '16é›†', 'æ½œè¡Œ åˆ˜å¾·å 2023': '3.14', 'çƒˆç„°': '40é›†', 'å®£æ­¦é—¨': '41é›†', 'ä»Šæ™šå¼€æ”¾éº¦ç¬¬2å­£': '3.13', 'èŠ±é—´ä»¤': '32é›†', 'è°¢è°¢ä½ æ¸©æš–æˆ‘': '15é›†', 'ä¸å‡¤è¡Œ': '39é›†', 'å°æ—¥å­': '27é›†', 'æ‰§ç¬”': '24é›†', 'è¿½é£è€…': '38é›†', 'æ¬¢ä¹é¢‚5': '34é›†', 'é¥ä¸å¯åŠçš„çˆ±': '3.36', 'ä¹˜é£è¸æµª': '2024é›†', 'ä»Šå¤©çš„å¥¹ä»¬': '24é›†', 'ç™¾ç‚¼æˆç¥': '78é›†', 'æ­¥æ­¥å€¾å¿ƒ': '28é›†', 'çŒœçŒœæˆ‘æ˜¯è°': '24é›†', 'æˆ‘ç‹¬è‡ªå‡çº§': '12é›†', 'æƒœèŠ±èŠ·': '2024', 'ä¸å¤Ÿå–„è‰¯çš„æˆ‘ä»¬': '08é›†', 'çº¢è¡£é†‰': '26é›†', 'ç›’å­é‡Œçš„çŒ«': '5.26', 'æ‰‹æœ¯ç›´æ’­é—´': '28é›†', 'æµ·è´¼ç‹': '1103é›†', 'è¯›ä»™2024': '37é›†', 'å“ˆå“ˆå“ˆå“ˆå“ˆ4': '5.26', 'å°è°¢å°”é¡¿ç¬¬7å­£': '06é›†', 'åˆè§é€é¥': '40é›†', 'éš¾å¯»': '28é›†', 'æ‹çˆ±å…„å¦¹': '16é›†', 'å…¬å¯“404': '08é›†', 'ä¸ƒäººçš„å¤æ´»': '16é›†', 'ç¾å¥½ä¸–ç•Œ': '14é›†', 'é€†å¤©å¥‡æ¡ˆ2': '30é›†', 'æ‰¿æ¬¢è®°': '2024', 'åŸä¸­ä¹‹åŸ': '40é›†', 'æ˜¥è‰²å¯„æƒ…äºº': '21é›†', 'èƒŒç€å–„å®°è·‘': '16é›†', 'è¥¿è¡Œçºª5': '36é›†', 'è¥¿è¡Œçºªåˆé›†': '4.26', 'é‰´ç½ªå¥³æ³•åŒ»ä¹‹é­‡å§‹': '24é›†', 'çˆ±åœ¨å¤©æ‘‡åœ°': '24é›†', 'çˆ±åœ¨å¤©æ‘‡åœ°åŠ¨æ—¶': '24é›†', 'ç§åœ°å§2': '5.25', 'æƒ…é¡': '92é›†', 'ç‚¼æ°”3000å±‚å¼€å±€æ”¶å¥³å¸ä¸ºå¾’': '78é›†', 'å¾®æš—ä¹‹ç«': '28é›†', 'æˆ‘ä»€ä¹ˆæ—¶å€™æ— æ•Œäº†': '94é›†', 'äº²çˆ±çš„å®‹å°å§': '69é›†', 'ä¹Œé¾™å­•äº‹': '76é›†', 'æ¸¸å­å½’å®¶': '88é›†', 'æ€»è£å¤«äººä¸ºä½•é‚£æ ·': '79é›†', 'æ–°ç”Ÿ': '10é›†', 'æˆ‘çš„é˜¿å‹’æ³°': '08é›†', 'æ­¤åˆ»æ— å£°': '20é›†', 'å“ˆå°”æ»¨ä¸€ä¹å››å››': '40é›†', 'ä¸å¯å‘Šäºº': '1080', 'åº†ä½™å¹´ç¬¬äºŒå­£': '31é›†', 'å®¶æ—è£è€€ä¹‹ç»§æ‰¿è€…': '1080'}
#print(send_update_to_group(movie_update_data, "https://moviespace02.com", True))
#print(send_update_to_group(movie_update_data, "https://affdz.com"))
#print(get_latest_postid(1500, "https://affdz.com"))
#print(movie_update_data)
#print(search_movie(["https://moviespace02.com", "https://moviespace01.com"], "ä»™é€†", True, False, False))
#print(search_movie(["https://moviespace02.com", "https://moviespace01.com"], "è²èŠ±æ¥¼", True, False, False))
#print(get_latest_postid(1, "https://moviespace02.com"))
#if __name__ == "__main__":
#    print(search_movie("https://affdz.com", "å¤©å®˜èµç¦ç¬¬äºŒå­£"))
